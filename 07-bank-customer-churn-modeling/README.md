
# Отток клиентов банка

## Описание проекта
Из «Бета-Банка» стали уходить клиенты. Каждый месяц. Немного, но заметно. Банковские маркетологи посчитали: сохранять текущих клиентов дешевле, чем привлекать новых.
<br>Нужно спрогнозировать, уйдёт клиент из банка в ближайшее время или нет. Нам предоставлены исторические данные о поведении клиентов и расторжении договоров с банком. 
<br>Необходимо построить модель с предельно большим значением *F1*-меры и довести метрику до 0.59. 
<br>Дополнительно измерим *AUC-ROC* и сравним её значение с *F1*-мерой.

## Этапы проекта
1. Загрузим и проведем предобработку данных: исправим типы данных, проанализируем пропуски и дубликаты, создадим новые столбцы.
2. Проведем исследовательский анализ: изучим долю ушедших и оставшися клиентов и рассмотрим парные корреляции признаков.
5. Обучим три вида модели: модель дерева, случайного леса и логистической регрессии без борьбы с дисбалансом.
7. Проведем обучение моделей с несколькими методами борьбы с дисбалансом:
  - взвешивание классов
  - upsampling
  - downsampling
  - изменение порогов
8. Протестируем лучшую модель и потроим ROC-кривую
9. Оценим важность фичей и сформулируем общие выводы
   
## Результат
<br>**Первичный исследовательский анализ показал:**
- корреляции между различными признаками/feature в данных не наблюдается. Но можно отследить некоторые корреляции к целевому признаку (оранжевым-ушедшие клиенты).
- количество ушедших клиентов в 4 раза меньше (20% и 80%), чем оставшихся - явно есть дисбаланс классов.
- чаще всего уходят клиенты в возрасте от 40 до 60 лет
- чаще всего уходят клиенты, имеющие 3-4 продукта (с 4 продуктами ушли практически все клиенты, судя по графикам)
- уход клиентов напрямую не коррелирует с наличием кредитной карты, сроком обслуживания в банке, балансом или зарплатой. С кредитным рейтингом в целом корреляция не наблюдается, однако клиенты с рейтингом меньше 400, как правило, уходят.

<br>**В ходе поиска модели с наилучшими показателями было сделано:**
- были рассмотрены три вида моделей с перебором гиперпараметров:
    - DecisionTreeClasissifier: перебор глубины дерева in range(2, 20)
    - RandomForestClassifier: перебор кол-ва деревьев in range(5,100) с глубиной деревьев 6 или 9
    - LogisticRegressor: перебор кол-ва итераций от in range(100,1000)
- аналогичные поиски моделей были проведены дополнительно с использованием техник балансировки классов:
    - взвешивание классов - гиперпараметр class_weight = 'balanced'
    - Upsampling - увеличение положительного класса до размера отрицательного
    - Downsampling - уменьшение отрицательного класса до размера положительного
- дополнительно был проведен эксперимент по изменению порогов для лучшей из найденных моделей

<br>Для каждой из моделей были рассчитаны следующие метрики: accuracy, recall, precision, f1, roc_auc. Все метрики по всем моделям сохранены в таблице models_results - можно проводить дополнительные исследования по изменению каждой метрики.
<br>Наилучшая из найденных моделей - модель случайного леса с глубиной 9 и 70деревьями, с апсэмплированным набором входных данных.
<br>Максимальное значение F1 на валидационной выборке - 0.63

<br>**Тестирование модели:**
- обучающая и валидационная выборки были объединены для наиболее эффективного обучения модели перед тестированием
- тестирование модели показало следующие результаты:
    - accuracy = 0.812
    - recall = 0.6926
    - precision = 0.5435
    - **f1 = 0.6091**
    - roc_auc = 0.8554
- также был построен график ROC-кривой, на котором видно, что кривая сильно выше, чем кривая случайной модели, что говорит о высоком качестве полученной модели случайного леса.


## Технические параметры среды
Библиотеки: pandas, numpy, sklearn, matplotlib, seaborn
<br>Версия Python: 3.9.5

